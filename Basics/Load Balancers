A load balancer is a component that distributes incoming traffic across multiple backend servers. Without it, all traffic would hit a single server, which risks overload and single point of failure.
Scaling vertically (adding CPU/RAM to one server) only goes so far and gets very expensive. Instead, we scale horizontally by adding more servers, and a load balancer makes this possible. It improves performance, avoids downtime, and makes scaling easier.
Modern load balancers also do more than just traffic distribution — they perform health checks to stop sending traffic to unhealthy servers, and they can provide features like SSL/TLS termination, caching, or session stickiness depending on the level (L4 vs L7).
So in short, a load balancer is central to building scalable, highly available systems.


Health Checks: LB periodically pings servers, removes bad ones from the pool.
High Availability: LBs themselves are replicated (multiple LBs in active-active or active-passive mode).
Cost justification: Horizontal scaling with LB gives flexibility — you can add/remove servers dynamically instead of paying huge costs upfront.
Performance: In modern data centers, the LB-to-server latency is negligible.

There are 2 types of load balancers: Level 4 and Level 7

Layer 4 (L4):
Operates at the transport layer, using IP addresses and ports to distribute traffic across servers.
It’s simple and fast, focusing on performance rather than flexibility.
L4 does not inspect the content of the requests—its primary job is to forward traffic efficiently.

Layer 7 (L7):
Operates at the application layer and can make routing decisions based on HTTP headers, URLs, cookies, or even request content.
L7 provides more features and flexibility, including:
TLS termination: The client establishes a secure connection with the load balancer, which handles encryption/decryption, offloading this work from the servers.
Health checks: The load balancer can check if a server is active and route traffic accordingly, often by probing application endpoints.
Session persistence: Ensures that requests from the same client go to the same server when necessary.
Caching: Can store sessions or frequently accessed content to improve performance.


The load balancer can be hardware loadbalancer or a software loadbalancer:

There are different ways to create a loadbalancer,
We can take an EC2 instance and convert that into a load balancer by running NGINX on it, or we can rent a baremetal loadbalancer from  datacenter
Or use a cloud load balancer like ALB or NLB.

IT all depends on the scenarios where im going to use these.
“In the early stages, I’d go cloud-native to focus on product development and marketing rather than infrastructure. 
Cloud scales automatically with traffic, so we only pay for what we use. If the product proves successful and traffic grows predictably, 
we can later evaluate self-managed servers for cost optimization or special performance needs.”

“NGINX is event-driven and non-blocking, so a single worker can handle thousands of TCP connections asynchronously. 
Unlike traditional thread-per-connection servers, NGINX uses a master-worker architecture and an event loop, making it extremely memory- and CPU-efficient, 
capable of handling millions of concurrent users.”
